---
title: "Appendix S5: No-analogue Experiment"
output: 
  pdf_document:
    citation_package: natbib
    number_sections: true    
header-includes: 
  \usepackage{natbib}
  \renewcommand{\thesection}{S5.\arabic{section}}  
bibliography: multivariate-gaussian-process.bib
pandoc_options: ["--biblatex"]
csl: institute-mathematical-statistics.csl
---

# Model performance under no-analogs for testate amoebae data

We conduct a pseudo-analog simulation using the testate data to test the hypothesis that the WA and MAT are sensitive to a lack of analogs. We conduct the experiment by filtering out a proportion of analogs approximately equal in size to a 12-fold cross-validation hold-out size which have the greatest minimum square chord distance from the calibration set. Figure \ref{fig:no-analog-testate} shows the distribution of square-chord distance for the no-analog training and prediction datasets, demonstrating that the training dataset has fewer nearby analogs in the prediction dataset with respect to square chord distance. Using the testate data, we train the model using the closest analogs and predict the unobserved water table depth on the non-analog hold-out data. 

The results from the no-analog simulation study in Table \ref{tab:app-one} show that the proposed MVGP and GAM model show reduced loss of predictive skill under the no analog scenario when compared to WA and MAT. The functional response based methods (MVGP, GAM, and MLRC) do not show as large a decrease in predictive performance because the functional response framework can handle extrapolations to novel compositions more naturally by leveraging the latent functional relationship to the covariate. Modeling the functional response allows for accommodation of non-analog compositions more easily because the interactions among species can be interpolated given the latent response, allowing for prediction using compositions which are not close in square chord distance to the training data.  We still see a low empirical coverage rate for MVGP and GAM on the testate amoeba data, suggesting that there is likely overdispersion or other characteristics in the data that are not explained by the MVGP and GAM methods.



\begin{figure}
\centering\includegraphics[width=1.0\linewidth]{testate-analog-distance.png}
\caption{Plot of square chord distance distributions for the training and prediction data for the testate data. The simulated no-analogs square chord distribution is centered at a higher value than the training dataset.}
\label{fig:no-analog-testate}
\end{figure}


\begin{table}
\centering
\caption{Results for predicting unobserved non-analog covariate values using the testate amoeba data. Smaller MSPE, MAE, and CRPS values indicate better model performance. Coverage values closer to the nominal 95\% credible interval indicate better model performance.}\label{tab:app-one}
\input{../../../results/appendix-booth}
\end{table}



# Model performance under no-analogs for pollen data

We also conduct a pseudo-analog simulation using the pollen data to test the hypothesis that the WA and MAT are sensitive to a lack of analogs. Like the testate amoeba data, we conduct the experiment by filtering out a proportion of analogs approximately equal in size to a 12-fold cross-validation hold-out size. The held-out data are those which have the greatest minimum square chord distance from the calibration set. Figure \ref{fig:no-analog-pollen} shows the distribution of square-chord distance for the no-analog training and prediction datasets, demonstrating that the training dataset has fewer nearby analogs in the prediction dataset with respect to square chord distance. Using the pollen data, we train the model using the closest analogs and predict the unobserved average July Temperature on the non-analog hold-out data. 

The results from the no-analog simulation study in Table \ref{tab:app-two} show that the proposed MVGP and GAM model show reduced loss of predictive skill under the no analog scenario when compared to WA and MAT. 


The functional response based methods (MVGP, GAM, and MLRC) do not show as large a decrease in predictive performance because the functional response framework can handle extrapolations to novel compositions more naturally by leveraging the latent functional relationship to the covariate. Modeling the functional response allows for accommodation of non-analog compositions more easily because the interactions among species can be interpolated given the latent response, allowing for prediction using compositions which are not close in square chord distance to the training data.  We see a better empirical coverage rate for MVGP and GAM on the pollen data, suggesting that these models are better calibrated to the pollen data.

\begin{figure}
\centering\includegraphics[width=1.0\linewidth]{pollen-analog-distance.png}
\caption{Plot of square chord distance distributions for the training and prediction data for the pollen dataset. The simulated no-analogs square chord distribution is centered at a higher value than the training dataset.}
\label{fig:no-analog-pollen}
\end{figure}


\begin{table}
\centering
\caption{Results for predicting unobserved non-analog covariate values using the pollen data. Smaller MSPE, MAE, and CRPS values indicate better model performance. Coverage values closer to the nominal 95\% credible interval indicate better model performance.}
\label{tab:app-two}
\input{../../../results/appendix-pollen}
\end{table}

# Discussion
Based on the small empirical study presented above, there is evidence that MVGP is less sensitive to a lack of analogs than WA or MAT. We see that the functional response based methods (MVGP, GAM, and MLRC) show less sensitivity to the no-analog problem than the transfer function methods of WA and MAT. The no-analog problem arises in many paleoclimate reconstructions and the implications on the reconstruction are not well understood \citep{jackson2004modern}. Modern methods that model the latent functional response like MVGP show evidence of being robust to this problem and further investigation is needed.


# R code and setup

```{r, warning=FALSE, message=FALSE, echo=TRUE}
set.seed(11)
library(BayesComposition)
library(knitr)
library(ggplot2)
library(rioja)
library(analogue)
library(here)
library(snowfall)
library(gridExtra)
library(dplyr)
library(here)
library(xtable)

## change these for your file structure

## output directory for MCMC output
save_directory <- "~/Google Drive/mvgp-test/"
## directory for MCMC monitoring output
progress_directory <- "./mvgp-test/"
```

# Testate amoebae data

First we load the testate amoebae data and subset to only include the Booth 2008 data and remove rare species (species with fewer that 500 total counts).

```{r load-data}
# Load Testate Data and R code - Booth 2008
raw_data <- read.csv(  
  file = here::here("data", 
                    "North American Raw Testate - Paleon 2017-Sheet1.csv"), 
                     skip=6)

## subset to Booth 2008
raw_data <- raw_data[1:378, ]
y <- raw_data[, 12:85]
X <- raw_data$WTD..cm.

## Subset to Booth 2008 data
N <- 356
N_obs <- 356
y <- y[1:356, ]
X <- X[1:356]

## join species

y <- join_testate_booth(y)
# source("~/testate/data/join-testate-booth.R")

## remove zeros
no_obs <- which(apply(y, 2, sum) == 0)

## down to 47 species
y <- y[, -no_obs]

## Subset rare species, only keeping those with over 500 total count
y <- y[, apply(y, 2, sum) > 500]
# y <- y[, colSums(y) > 100]

mean_X <- mean(X)
sd_X <- sd(X)
X <- (X - mean_X) / sd_X
N <- nrow(y)

N <- dim(y)[1]
d <- dim(y)[2]

## transform the data to percentages for use in transfer function models
y_prop <- y
for (i in 1:N) {
  y_prop[i, ] <- y_prop[i, ] / sum(y_prop[i, ])
}
```

## Generate testate amoebae non-analogs

Next, we generate pseudo-non-analogs by selecting approximately 1/12th of the samples with the largest minimum pairwise square chord distance with all of the other samples. We call these our non-analogs. 


```{r sim-analog, message=FALSE, warning=FALSE, cache=TRUE, out.width="95%"}
## Simulate a "no-analog" situation
modMATDist <- MAT(y_prop, X, k=20, lean=FALSE)

mod_dists <- tibble(set = "mod", distance = as.vector(modMATDist$dist.n))

## 0.275 cut-off chosen so the validation set is approximately the same size
## as one of the 12-fold cross-validation sets
analog_idx <- which(apply(modMATDist$dist.n, 1, min)  > 0.275)
# length(analog_idx) ## 32
# 356/12  ## 29.67

y_train <- y[-analog_idx, ]
y_test <- y[analog_idx, ]
X_train <- X[-analog_idx]
X_test <- X[analog_idx]
y_train_prop <- y_train
for (i in 1:nrow(y_train)) {
  y_train_prop[i, ] <- y_train_prop[i, ] / sum(y_train_prop[i, ])
}
y_test_prop <- y_test
for (i in 1:nrow(y_test)) {
  y_test_prop[i, ] <- y_test_prop[i, ] / sum(y_test_prop[i, ])
}
N_pred <- dim(y_test)[1]

modMATDistTrain <- MAT(y_train_prop, X, k=20, lean=FALSE)
predMATDist <- predict(modMATDistTrain, y_test_prop, k=20, sse=TRUE, n.boot=1000, verbose=FALSE)
mod_dists <- tibble(set = "mod", distance = as.vector(modMATDistTrain$dist.n))
pred_dists <- tibble(set = "pred", distance = as.vector(predMATDist$dist.n))

MAT_dists <- bind_rows(mod_dists, pred_dists)


MAT_dists_plot <- ggplot(MAT_dists, aes(distance, fill=set)) +
  geom_density(alpha=0.7, adjust=1, colour="grey50") + 
  labs(x="square chord distance") + 
  scale_fill_discrete(name=element_blank(), breaks=c("mod", "pred"),
                      labels=c("Training Dataset", "Prediction Dataset")) +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5, size = 30),
        axis.text.x = element_text(size = 22), 
        axis.text.y = element_text(size = 22),
        axis.title.x = element_text(size = 22), 
        axis.title.y = element_text(size = 22),
        legend.text=element_text(size=30)) +
    ggtitle("Testate Data Non-analogs")

png(file=paste0(save_directory, "testate-analog-distance.png"),
    width=18, height=6, units="in", res=400)
MAT_dists_plot
invisible(dev.off())
include_graphics(paste0(save_directory, "testate-analog-distance.png"))
```

## Plot the analog and non-analog test data 

The training data and the testing (held-out) data are plotted below.


```{r plot-analog-data}
analogPlotData <- data.frame(
  species = as.factor(rep(colnames(y), each=length(X_train))),
  Count   = c(as.matrix(y_train_prop)), 
  Wetness = rep(X_train * sd_X + mean_X, times=dim(y_train_prop)[2]))

noanalogPlotData <- data.frame(
  species = as.factor(rep(colnames(y), each=length(X_test))),
  Count   = c(as.matrix(y_test_prop)), 
  Wetness = rep(X_test * sd_X + mean_X, times=dim(y_test_prop)[2]))

gp1 <- ggplot(analogPlotData, aes(x=Wetness, y=Count, color=species, group=species)) +
  geom_point(alpha=0.25) +
  theme(legend.position="none") + ggtitle("Analog Training Data") + 
  labs(x="Water Table Depth", y="Composition") + 
  theme(plot.title=element_text(size=24, face="bold", hjust=0.5)) + 
  theme(axis.text.x = element_text(size = 22), 
        axis.text.y = element_text(size = 22),
        axis.title.x = element_text(size = 22), 
        axis.title.y = element_text(size = 22))
gp2 <- ggplot(noanalogPlotData, 
              aes(x=Wetness, y=Count, color=species, group=species)) +
  geom_point(alpha=0.25) +
  theme(legend.position="none") + ggtitle("Non-Analog Test Data") + 
  labs(x="Water Table Depth", y="Composition") + 
  theme(plot.title=element_text(size=24, face="bold", hjust=0.5)) + 
  theme(axis.text.x = element_text(size = 22), 
        axis.text.y = element_text(size = 22),
        axis.title.x = element_text(size = 22), 
        axis.title.y = element_text(size = 22))
multiplot(gp1, gp2, cols=1)
```

## Fitting MVGP to testate amoebae data

We define the parameters for fitting the MVGP model to the testate amoebae data. First we define a sequence of 30 equally-spaced knots over which the predictive process is defined, extending the range of the predictive process knots beyond the range of the data to avoid boundary effects. We have found the reconstructions to be not very sensitive to the choice and spacing of the knots. 

```{r setup-fit}
y <- as.matrix(y)
n_knots <- 30
X_knots <- seq(min(X, na.rm=TRUE)-1.25*sd(X, na.rm=TRUE), 
               max(X, na.rm=TRUE)+1.25*sd(X, na.rm=TRUE), length=n_knots)
```

We fit the MVGP model to the testate amoebae data for 200,000 MCMC iterations discarding the first 50,000 iterations as burn-in. We thin every 150 of the remaining 150,000 MCMC samples for 4 parallel chains resulting in 4,000 posterior samples. We fit the MVGP model with an exponential covariance function where we estimate the correlation in functional response. We do not include an overdispersion covariance $\boldsymbol{\Sigma}_{\varepsilon}$ in the model. To get the correlation in functional responses to converge, we set the prior $\mathbf{R} \sim \operatorname{LKJ}(\eta = 8)$, providing some shrinkage of the functional correlations towards 0.

```{r fit-testate}
params <- list(
  n_adapt                    = 50000,
  n_mcmc                     = 150000,
  n_thin                     = 150, 
  correlation_function       = "exponential", 
  likelihood                 = "dirichlet-multinomial", 
  function_type              = "gaussian-process",
  multiplicative_correlation = TRUE,
  eta                        = 8,
  additive_correlation       = FALSE,
  n_chains                   = 4,
  n_cores                    = 4,
  n_knots                    = n_knots,
  X_knots                    = X_knots)

## Fit no-analog using B-spline model
if (file.exists(paste0(save_directory, "fit-mvgp-testate-no-analog2.RData"))) {

  ## load mcmc
  load(paste0(save_directory, "fit-mvgp-testate-no-analog2.RData"))
} else {
  
  ## potentially long running MCMC code
  out <- fit_compositional_data(
    y                  = y_train,
    X                  = X_train,
    params             = params, 
    progress_directory = progress_directory,
    progress_file      = "fit-mvgp-testate-no-analog2.txt")
    
  save(out, file = paste0(save_directory, "fit-mvgp-testate-no-analog2.RData"))
}
```

```{r, echo=FALSE, include=FALSE}
Rhat <- make_gelman_rubin(out)
```

### MVGP convergence diagnostics
After fitting the model, the posterior samples are generated and MCMC convergence is assessed using the Gelman-Rubin $\hat{R}$ statistic. We see the distribution of $\hat{R}$ statistics is close to 1 with the largest $\hat{R} = `r max(na.omit(Rhat))` \leq 1.1$ at an acceptable value (\citet{gelman2013bayesian}, pp.287).  

```{r}
## extract posterior samples
samples <- extract_compositional_samples(out)

mu_post <- samples$mu
eta_star_post <- samples$eta_star
zeta_post <- samples$zeta
alpha_post <- samples$alpha
Omega_post <- samples$Omega
phi_post <- samples$phi
tau2_post <- samples$tau2
R_post <- samples$R
R_tau_post <- samples$R_tau
xi_post <- samples$xi

n_samples <- nrow(mu_post)


Rhat <- make_gelman_rubin(out)
layout(matrix(1:9, 3, 3))
hist(Rhat[grepl("eta", names(Rhat))], main = "Rhat for eta")
hist(Rhat[grepl("mu", names(Rhat))], main = "Rhat for mu")
hist(Rhat[grepl("alpha", names(Rhat))], main = "Rhat for alpha")
hist(Rhat[grepl("zeta", names(Rhat))], main = "Rhat for zeta")
hist(Rhat[grepl("tau2", names(Rhat))], main = "Rhat for tau2")
hist(Rhat[grepl("xi", names(Rhat))], main = "Rhat for xi")
hist(Rhat[grepl("phi", names(Rhat))], main = "Rhat for phi")
hist(Rhat, main="All parameters")
```

### Generating MVGP predictions

Using the elliptical slice sampler \citep{murray2010elliptical}, we generate posterior predictions for the unobserved water table depth using the posterior samples estimated from the testate amoebae calibration model. These estimates can be generated either jointly with the calibration model or estimated using a two-stage approach as presented here. We have found, there is little practical difference in model performance between joint and two-stage estimation and follow the convention in the transfer function literature of using two-stage estimation.

```{r}
## Note, there might be some initial warnings about ESS shrunk to the
##    current position for the first few iterations. This is not a 
##    major concern as long as the warnings don't continue.
if (file.exists(paste0(save_directory, "predict-mvgp-testate-no-analog2.RData"))) {
  load(paste0(save_directory, "predict-mvgp-testate-no-analog2.RData"))
} else {
  pred <- predict_compositional_data(
    y_test, 
    X_train,
    params, 
    samples, 
    progress_directory, 
    "predict-mvgp-testate-no-analog2.txt")
  save(pred, file = paste0(save_directory, "predict-mvgp-testate-no-analog2.RData"))
}
```

### Plot MVGP predictions

The predictions for the chosen non-analog missing covariate observations are shown below. The held-out covariate values are shown in red dots and the posterior distribution shown as a violin plot where the width of the violin is proportional to the posterior density. The posterior mean is shown in blue and the posterior median is shown in orange. In general, the posterior distributions are doing a good job of estimating the non-analog water table depth covariate.

```{r, cache = TRUE}
## sorted to increasing values for ease of display
idx <- order(X_test)
## randomly choose 25 observations to display

n_samples <- length(pred$X[, 1])

sim_df <- data.frame(
  covariate   = c(pred$X[, idx] * sd_X + mean_X),
  mean        = rep(apply(pred$X[, idx] * sd_X + mean_X, 2, mean), 
                    each = n_samples),
  median      = rep(apply(pred$X[, idx], 2, median) * sd_X + mean_X,
                    each = n_samples),
  observation = factor(rep(1:length(analog_idx), each = n_samples )),
  truth       = rep(X_test[idx] * sd_X + mean_X,
                    each = n_samples))

ggplot(sim_df, aes(observation, covariate)) +
  geom_violin(position="identity") +
  geom_point(aes(observation, truth), color="red") +
  geom_point(aes(observation, mean), color="blue") +
  geom_point(aes(observation, median), color="orange") +
  scale_x_discrete(breaks=seq(5, length(analog_idx), 5)) + 
  labs(x="Observation", y="Predicted Water Table Depth") +
  ggtitle("MVGP prediction for testate data")
```


## GAM model fit to testate amoebae data


We fit the GAM model for 200,000 MCMC iterations discarding the first 50,000 iterations as burn-in. We thin every 150 of the remaining 150,000 MCMC samples for 4 parallel chains resulting in 4,000 posterior samples. We used the default setting in `BayesComposition` resulting in a cubic B-spline model with 6 degrees of freedom.

```{r}
params <- list(
  n_adapt                    = 50000,
  n_mcmc                     = 150000,
  n_thin                     = 150, 
  likelihood                 = "dirichlet-multinomial", 
  function_type              = "basis",
  n_chains                   = 4,
  n_cores                    = 4)

## Fit no-analog using B-spline model
if (file.exists(paste0(save_directory, "fit-gam-testate-no-analog.RData"))) {
  ## load mcmc
  load(paste0(save_directory, "fit-gam-testate-no-analog.RData"))
} else {
  
  ## potentially long running MCMC code
  out <- fit_compositional_data(
    y                  = y_train,
    X                  = X_train,
    params             = params, 
    progress_directory = progress_directory,
    progress_file      = "fit-gam-testate-no-analog.txt")
    
  save(out, file = paste0(save_directory, "fit-gam-testate-no-analog.RData"))
}
```

```{r, echo=FALSE, include=FALSE}
Rhat <- make_gelman_rubin(out)
```

### GAM convergence diagnostics

After fitting the model, the posterior samples are generated and MCMC convergence is assessed using the Gelman-Rubin $\hat{R}$ statistic. We see the distribution of $\hat{R}$ statistics is close to 1 with the largest $\hat{R} = `r max(na.omit(Rhat))` \leq 1.1$ at an acceptable value (\citet{gelman2013bayesian}, pp.287). 

```{r, eval=TRUE}
## extract posterior samples
samples <- extract_compositional_samples(out)
beta_post <- samples$beta
mu_beta_post <- samples$mu_beta
Sigma_beta_post <- samples$Sigma_beta
alpha_post <- samples$alpha

n_samples <- nrow(beta_post)

## Calculate Gelman-Rubin convergence statistic
Rhat <- make_gelman_rubin(out)
layout(matrix(1:6, 3, 2))
hist(Rhat, main="All parameters")
hist(Rhat[grepl("beta", names(Rhat))], main = "Rhat for beta")
hist(Rhat[grepl("alpha", names(Rhat))], main = "Rhat for alpha")
hist(Rhat[grepl("mu_beta", names(Rhat))], main = "Rhat for mu_beta")
hist(Rhat[grepl("Sigma_beta", names(Rhat))], main = "Rhat for Sigma_beta")
Rhat[!is.finite(Rhat)] <- NA
max(unlist(na.omit(Rhat)))
```

### Generate GAM predictions

```{r}
## Note, there might be some initial warnings about ESS shrunk to the
##    current position for the first few iterations. This is not a 
##    major concern as long as the warnings don't continue.
if (file.exists(paste0(save_directory, "predict-gam-testate-no-analog.RData"))) {
  load(paste0(save_directory, "predict-gam-testate-no-analog.RData"))
} else {
  pred_gam <- predict_compositional_data(
    y_test, 
    X_train,
    params, 
    samples, 
    progress_directory, 
    "predict-gam-testate-no-analog.txt")
  save(pred_gam, file = paste0(save_directory, "predict-gam-testate-no-analog.RData"))
}
```

### Plot GAM predictions

The predictions for the chosen non-analog missing covariate observations are shown below. The held-out covariate values are shown in red dots and the posterior distribution shown as a violin plot where the width of the violin is proportional to the posterior density. The posterior mean is shown in blue and the posterior median is shown in orange. In general, the posterior distributions are not reconstructing the unobserved as well as the MVGP model.

```{r, cache = TRUE}
## sorted to increasing values for ease of display
idx <- order(X_test)
## randomly choose 25 observations to display

n_samples <- length(pred$X[, 1])

sim_df <- data.frame(
  covariate   = c(pred_gam$X[, idx] * sd_X + mean_X),
  mean        = rep(apply(pred_gam$X[, idx] * sd_X + mean_X, 2, mean), 
                    each = n_samples),
  median      = rep(apply(pred_gam$X[, idx], 2, median) * sd_X + mean_X,
                    each = n_samples),
  observation = factor(rep(1:length(analog_idx), each = n_samples )),
  truth       = rep(X_test[idx] * sd_X + mean_X,
                    each = n_samples))

ggplot(sim_df, aes(observation, covariate)) +
  geom_violin(position="identity") +
  geom_point(aes(observation, truth), color="red") +
  geom_point(aes(observation, mean), color="blue") +
  geom_point(aes(observation, median), color="orange") +
  scale_x_discrete(breaks=seq(5, length(analog_idx), 5)) + 
  labs(x="Observation", y="Predicted Water Table Depth") +
  ggtitle("GAM prediction for testate no-analog data")
```


## Bummer model fit to testate amoebae data

We fit the BUMMER model for 200,000 MCMC iterations discarding the first 50,000 iterations as burn-in. We thin every 150 of the remaining 150,000 MCMC samples for 4 parallel chains resulting in 4,000 posterior samples. 

```{r}
params <- list(
  n_adapt                    = 50000,
  n_mcmc                     = 150000,
  n_thin                     = 150, 
  likelihood                 = "dirichlet-multinomial", 
  function_type              = "bummer",
  n_chains                   = 4,
  n_cores                    = 4)


## Fit no-analog using bummer model
if (file.exists(paste0(save_directory, "fit-bummer-testate-no-analog.RData"))) {

  ## load mcmc
  load(paste0(save_directory, "fit-bummer-testate-no-analog.RData"))
} else {
  
  ## potentially long running MCMC code
  out <- fit_compositional_data(
    y                  = y_train,
    X                  = X_train,
    params             = params, 
    progress_directory = progress_directory,
    progress_file      = "fit-bummer-testate-no-analog.txt")
    
  save(out, file = paste0(save_directory, "fit-bummer-testate-no-analog.RData"))
}
```

```{r, echo=FALSE, include=FALSE}
Rhat <- make_gelman_rubin(out)
```

### BUMMER convergence diagnostics

After fitting the model, the posterior samples are generated and MCMC convergence is assessed using the Gelman-Rubin $\hat{R}$ statistic. We see the distribution of $\hat{R}$ statistics is close to 1 with the largest $\hat{R} = `r max(na.omit(Rhat))` \leq 1.1$ at an acceptable value (\citet{gelman2013bayesian}, pp.287). 

```{r, eval=TRUE}
## extract posterior samples
samples <- extract_compositional_samples(out)
a_post <- samples$a
b_post <- samples$b
c_post <- samples$c
alpha_post <- samples$alpha

n_samples <- nrow(a_post)

## Calculate Gelman-Rubin convergence statistic
Rhat <- make_gelman_rubin(out)
layout(matrix(1:6, 3, 2))
hist(Rhat, main="All parameters")
hist(Rhat[grepl("a\\b", names(Rhat))], main = "Rhat for a")
hist(Rhat[grepl("b", names(Rhat))], main = "Rhat for b")
hist(Rhat[grepl("c", names(Rhat))], main = "Rhat for c")
hist(Rhat[grepl("alpha", names(Rhat))], main = "Rhat for alpha")
Rhat[!is.finite(Rhat)] <- NA
max(unlist(na.omit(Rhat)))
```

### Generate BUMMER predictions

Using the elliptical slice sampler \citep{murray2010elliptical}, we generate posterior predictions for the unobserved water table depth using the posterior samples estimated from the testate amoebae calibration model. These estimates can be generated either jointly with the calibration model or estimated using a two-stage approach as presented here. We have found, there is little practical difference in model performance between joint and two-stage estimation and follow the convention in the transfer function literature of using two-stage estimation.

```{r}
## Note, there might be some initial warnings about ESS shrunk to the
##    current position for the first few iterations. This is not a 
##    major concern as long as the warnings don't continue.
if (file.exists(paste0(save_directory, "predict-bummer-testate-no-analog.RData"))) {
  load(paste0(save_directory, "predict-bummer-testate-no-analog.RData"))
} else {
  pred_bummer <- predict_compositional_data(
    y_test, 
    X_train,
    params, 
    samples, 
    progress_directory, 
    "predict-bummer-testate-no-analog.txt")
  save(pred_bummer, file = paste0(save_directory, "predict-bummer-testate-no-analog.RData"))
}
```

### Plot BUMMER predictions

The predictions for the chosen non-analog missing water table depth observations are shown below. The held-out covariate values are shown in red dots and the posterior distribution shown as a violin plot where the width of the violin is proportional to the posterior density. The posterior mean is shown in blue and the posterior median is shown in orange. In general, the posterior distributions are not reconstructing the held-out test data as well as the MVGP model.

```{r, cache = TRUE}
## sorted to increasing values for ease of display
idx <- order(X_test)
## randomly choose 25 observations to display

n_samples <- length(pred_bummer$X[, 1])

sim_df <- data.frame(
  covariate   = c(pred_bummer$X[, idx] * sd_X + mean_X),
  mean        = rep(apply(pred_bummer$X[, idx] * sd_X + mean_X, 2, mean), 
                    each = n_samples),
  median      = rep(apply(pred_bummer$X[, idx], 2, median) * sd_X + mean_X,
                    each = n_samples),
  observation = factor(rep(1:length(analog_idx), each = n_samples )),
  truth       = rep(X_test[idx] * sd_X + mean_X,
                    each = n_samples))

ggplot(sim_df, aes(observation, covariate)) +
  geom_violin(position="identity") +
  geom_point(aes(observation, truth), color="red") +
  geom_point(aes(observation, mean), color="blue") +
  geom_point(aes(observation, median), color="orange") +
  scale_x_discrete(breaks=seq(5, length(analog_idx), 5)) + 
  labs(x="Observation", y="Predicted Water Table Depth") +
  ggtitle("BUMMER prediction for testate no-analog data")
```

## Fitting transfer function models to the non-analog testate amoebae data

Below, we fit the transfer function methods WA, MAT, and MLRC using the `R` `rioja` package.


```{r, message=FALSE, include=FALSE}
if (file.exists(paste0(save_directory, "other-models-testate-no-analog.RData"))) {
  load(paste0(save_directory, "other-models-testate-no-analog.RData")) 
} else {
  y_train_prop <- as.matrix(y_train_prop)
  
  ## WA reconstruction - subset to deal with all zero occurrence species
  zeros_idx <- which(apply(y_train_prop, 2, sum) == 0)
  if (length(zeros_idx) > 0) {
    modWA <- rioja::WA(y_train_prop[, - zeros_idx], X_train)
    predWA <- predict(modWA, y_test_prop[, - zeros_idx], sse=TRUE, nboot=1000)
  } else {
    ## no data to subset
    modWA <- rioja::WA(y_train_prop, X_train)
    predWA <- predict(modWA, y_test_prop, sse=TRUE, nboot=1000)      
  }
  
  pred_mu_WA <- predWA$fit[, 1]
  pred_sd_WA <- sqrt(predWA$v1.boot[, 1]^2 + predWA$v2.boot[1]^2)
  
  ## MLRC reconstruction - subset to deal with all zero occurrence species
  zeros_idx <- which(apply(y_train_prop, 2, sum) == 0)
  if (length(zeros_idx) > 0) {
    modMLRC <- rioja::MLRC(y_train_prop[, - zeros_idx], X_train)
    predMLRC <- predict(modMLRC, y_test_prop[, - zeros_idx],
                        sse=TRUE, nboot=1000)
  } else {
    modMLRC <- rioja::MLRC(y_train_prop, X_train)
    predMLRC <- predict(modMLRC, y_test_prop, sse=TRUE, nboot=1000)
  }
  
  pred_mu_MLRC <- predMLRC$fit[, 1]
  pred_sd_MLRC <- sqrt(predMLRC$v1.boot[, 1]^2 + predMLRC$v2.boot[1]^2)
  
  ## Modern analogue technique
  modMAT <- MAT(y_train_prop, X_train, k=20, lean=FALSE)
  predMAT <- predict(modMAT, y_test_prop, k=10, sse=TRUE, n.boot=1000)
  
  pred_mu_MAT <- predMAT$fit.boot[, 2]
  pred_sd_MAT <- sqrt(predMAT$v1.boot[, 2]^2+ predMAT$v2.boot[2])
  
  
  save(pred_mu_WA, pred_sd_WA, pred_mu_MLRC, pred_sd_MLRC, 
       pred_mu_MAT, pred_sd_MAT, 
       file = paste0(save_directory, "other-models-testate-no-analog.RData"))
}
```

### Generate non-analog predictive scores

After fitting all of the models under consideration, we compute the predictive scores for the testate amoebae data below.

```{r}
models <- c("MVGP", "GAM", "BUMMER", "WA", "MAT", "MLRC")
n_models <- length(models)
N_no_analog <- length(analog_idx)
coverage <- matrix(0, N_no_analog, n_models)
MSPE <- matrix(0, N_no_analog, n_models)
MAE <- matrix(0, N_no_analog, n_models)
CRPS <- matrix(0, N_no_analog, n_models)

MSPE[, 1] <- (X_test - apply(pred$X, 2, mean))^2
MSPE[, 2] <- (X_test - apply(pred_gam$X, 2, mean))^2
MSPE[, 3] <- (X_test - apply(pred_bummer$X, 2, mean))^2
MSPE[, 4] <- (X_test - pred_mu_WA)^2
MSPE[, 5] <- (X_test - pred_mu_MAT)^2
MSPE[, 6] <- (X_test - pred_mu_MLRC)^2

MAE[, 1] <- abs(X_test - apply(pred$X, 2, median))
MAE[, 2] <- abs(X_test - apply(pred_gam$X, 2, median))
MAE[, 3] <- abs(X_test - apply(pred_bummer$X, 2, median))
MAE[, 4] <- abs(X_test - pred_mu_WA)
MAE[, 5] <- abs(X_test - pred_mu_MAT)
MAE[, 6] <- abs(X_test - pred_mu_MLRC)

coverage[, 1] <- 
  (X_test > apply(pred$X, 2, quantile, prob=0.025)) & 
  (X_test < apply(pred$X, 2, quantile, prob=0.975))
coverage[, 2] <-
  (X_test > apply(pred_gam$X, 2, quantile, prob=0.025)) &
  (X_test < apply(pred_gam$X, 2, quantile, prob=0.975))
coverage[, 3] <-
  (X_test > apply(pred_bummer$X, 2, quantile, prob=0.025)) &
  (X_test < apply(pred_bummer$X, 2, quantile, prob=0.975))
coverage[, 4] <- 
  (X_test > (pred_mu_WA - 2 * pred_sd_WA)) &
  (X_test < (pred_mu_WA + 2 * pred_sd_WA))
coverage[, 5] <- 
  (X_test > (pred_mu_MAT - 2 * pred_sd_MAT)) &
  (X_test < (pred_mu_MAT + 2 * pred_sd_MAT))
coverage[, 6] <-
  (X_test > (pred_mu_MLRC - 2 * pred_sd_MLRC)) &
  (X_test < (pred_mu_MLRC + 2 * pred_sd_MLRC))

CRPS[, 1] <- makeCRPS(pred$X, X_test, dim(pred$X)[1])
CRPS[, 2] <- makeCRPS(pred_gam$X, X_test, dim(pred_gam$X)[1])
CRPS[, 3] <- makeCRPS(pred_bummer$X, X_test, dim(pred_bummer$X)[1])
CRPS[, 4] <- MAE[, 4]
CRPS[, 5] <- MAE[, 5]
CRPS[, 6] <- MAE[, 6]

colnames(MSPE) <- models
colnames(MAE) <- models
colnames(coverage) <- models
colnames(CRPS) <- models


results <- rbind(
  apply(CRPS, 2, mean), apply(MSPE, 2, mean),
  apply(MAE, 2, mean), 100*apply(coverage, 2, mean))
rownames(results) <- c("CRPS", "MSPE", "MAE", "95% CI coverage")
# print(xtable(t(results), digits=4),
#       file=here("results", "appendix-booth.tex"),
#       floating=FALSE)
```

```{r}
kable(t(results), digits = 4, format = "latex", longtable = FALSE)
```


# Non-analog Pollen Data

First, we load the data.

```{r readData-pollen, echo=TRUE, include=TRUE}
dat <- read.csv(here::here("data", "Reduced.Taxa.calibration.3.23.17.csv"),
                stringsAsFactors=FALSE, header=TRUE)
N <- length(dat$ACERX[-1])
d <- 16
y <- matrix(c(as.numeric(dat$ACERX[-1]), as.numeric(dat$BETULA[-1]), 
              as.numeric(dat$Sum.Other.Conifer[-1]), as.numeric(dat$LARIXPSEU[-1]), 
              as.numeric(dat$Sum.Other.Deciduous[-1]), as.numeric(dat$FAGUS[-1]), 
              as.numeric(dat$FRAXINUX[-1]), as.numeric(dat$Sum.Other.Herbaceous[-1]), 
              as.numeric(dat$Sum.Prairie.Herbs[-1]), as.numeric(dat$Other[-1]), 
              as.numeric(dat$PICEAX[-1]), as.numeric(dat$PINUSX[-1]), 
              as.numeric(dat$QUERCUS[-1]), as.numeric(dat$TILIA[-1]), 
              as.numeric(dat$TSUGAX[-1]), as.numeric(dat$ULMUS[-1])), N, d)

## Adjust for half counts
y <- ceiling(y)
colnames(y) <- names(dat)[5:20]

X_annual <- as.numeric(dat$tmean_annual[-1])
X <- as.numeric(dat$tmean_07[-1])

mean_X <- mean(X)
sd_X <- sd(X)
X <- (X - mean_X) / sd_X

## transform the data to percentages for use in transfer function models
y_prop <- y
for (i in 1:N) {
  y_prop[i, ] <- y_prop[i, ] / sum(y_prop[i, ])
}
```



## Generate pollen non-analogs

Next, we generate pseudo-non-analogs by selecting approximately 1/12th of the samples with the largest minimum pairwise square chord distance with all of the other samples. We call these our non-analogs. 

```{r sim-no-analog-pollen, warning=FALSE, message=FALSE, cache=TRUE, out.width="95%"}
## Simulate a "no-analog" situation
modMATDist <- MAT(as.data.frame(y_prop), X, k=10, lean=FALSE)

mod_dists <- tibble(set = "mod", distance = as.vector(modMATDist$dist.n))

## 0.008 cut-off chosen so the validation set is approximately the same size
## as one of the 12-fold cross-validation sets
analog_idx <- which(apply(modMATDist$dist.n, 1, min)  > 0.08)
# length(analog_idx) ## 14
# 152/12  ## 12.67

y_train <- y[-analog_idx, ]
y_test <- y[analog_idx, ]
X_train <- X[-analog_idx]
X_test <- X[analog_idx]
y_train_prop <- y_train
for (i in 1:nrow(y_train)) {
  y_train_prop[i, ] <- y_train_prop[i, ] / sum(y_train_prop[i, ])
}
y_test_prop <- y_test
for (i in 1:nrow(y_test)) {
  y_test_prop[i, ] <- y_test_prop[i, ] / sum(y_test_prop[i, ])
}
N_pred <- dim(y_test)[1]


modMATDistTrain <- MAT(as.data.frame(y_train_prop), X_train, k=10, lean=FALSE)
predMATDist <- predict(modMATDistTrain, as.data.frame(y_test_prop),
                       k=10, sse=TRUE, n.boot=1000)
mod_dists <- tibble(set = "mod", distance = as.vector(modMATDistTrain$dist.n))
pred_dists <- tibble(set = "pred", distance = as.vector(predMATDist$dist.n))

MAT_dists <- bind_rows(mod_dists, pred_dists)


MAT_dists_plot <- ggplot(MAT_dists, aes(distance, fill=set)) +
  geom_density(alpha=0.7, adjust=1, colour="grey50") + 
  labs(x="square chord distance") + 
  scale_fill_discrete(name=element_blank(), breaks=c("mod", "pred"),
                      labels=c("Training Dataset", "Prediction Dataset")) +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5, size = 30),
        axis.text.x = element_text(size = 22), 
        axis.text.y = element_text(size = 22),
        axis.title.x = element_text(size = 22), 
        axis.title.y = element_text(size = 22),
        legend.text=element_text(size=30)) +
  ggtitle("Pollen Data Non-analogs")


png(file=paste0(save_directory, "pollen-analog-distance.png"),
    width=18, height=6, units="in", res=400)
MAT_dists_plot
invisible(dev.off())
include_graphics(paste0(save_directory, "pollen-analog-distance.png"))
```


## MVGP model fit to pollen non-analog data.


We define the parameters for fitting the MVGP model to the pollen data. First we define a sequence of 30 equally-spaced knots over which the predictive process is defined, extending the range of the predictive process knots beyond the range of the data to avoid boundary effects. We have found the reconstructions to be not very sensitive to the choice and spacing of the knots. 

```{r setup-fit-pollen}
y <- as.matrix(y)
n_knots <- 30
X_knots <- seq(min(X, na.rm=TRUE)-1.25*sd(X, na.rm=TRUE), 
               max(X, na.rm=TRUE)+1.25*sd(X, na.rm=TRUE), length=n_knots)
```

We fit the MVGP model to the pollen data for 200,000 MCMC iterations discarding the first 50,000 iterations as burn-in. We thin every 150 of the remaining 150,000 MCMC samples for 4 parallel chains resulting in 4,000 posterior samples. We fit the MVGP model with an exponential covariance function where we estimate the correlation in functional response. We do not include an overdispersion covariance $\boldsymbol{\Sigma}_{\varepsilon}$ in the model. 


```{r fit-pollen}
params <- list(
  n_adapt                    = 50000,
  n_mcmc                     = 150000,
  n_thin                     = 150, 
  correlation_function       = "exponential", 
  likelihood                 = "dirichlet-multinomial", 
  function_type              = "gaussian-process",
  multiplicative_correlation = TRUE,
  additive_correlation       = FALSE,
  n_chains                   = 4,
  n_cores                    = 4,
  n_knots                    = n_knots,
  X_knots                    = X_knots)

## Fit no-analog using MVGP model
if (file.exists(paste0(save_directory, "fit-mvgp-pollen-no-analog2.RData"))) {

  ## load mcmc
  load(paste0(save_directory, "fit-mvgp-pollen-no-analog2.RData"))
} else {
  
  ## potentially long running MCMC code
  out <- fit_compositional_data(
    y                  = y_train,
    X                  = X_train,
    params             = params, 
    progress_directory = progress_directory,
    progress_file      = "fit-mvgp-pollen-no-analog2.txt")
    


  save(out, file = paste0(save_directory, file = "fit-mvgp-pollen-no-analog2.RData"))
}
```


```{r, echo=FALSE, include=FALSE}
Rhat <- make_gelman_rubin(out)
```

### MVGP convergence diagnostics

After fitting the model, the posterior samples are generated and MCMC convergence is assessed using the Gelman-Rubin $\hat{R}$ statistic. We see the distribution of $\hat{R}$ statistics is close to 1 with the largest $\hat{R} = `r max(na.omit(Rhat))` \leq 1.1$ at an acceptable value (\citet{gelman2013bayesian}, pp.287). 

```{r}
## extract posterior samples
samples <- extract_compositional_samples(out)

mu_post <- samples$mu
eta_star_post <- samples$eta_star
zeta_post <- samples$zeta
alpha_post <- samples$alpha
Omega_post <- samples$Omega
phi_post <- samples$phi
tau2_post <- samples$tau2
R_post <- samples$R
R_tau_post <- samples$R_tau
xi_post <- samples$xi

n_samples <- nrow(mu_post)

Rhat <- make_gelman_rubin(out)
layout(matrix(1:9, 3, 3))
hist(Rhat[grepl("eta", names(Rhat))], main = "Rhat for eta")
hist(Rhat[grepl("mu", names(Rhat))], main = "Rhat for mu")
hist(Rhat[grepl("alpha", names(Rhat))], main = "Rhat for alpha")
hist(Rhat[grepl("zeta", names(Rhat))], main = "Rhat for zeta")
hist(Rhat[grepl("tau2", names(Rhat))], main = "Rhat for tau2")
hist(Rhat[grepl("xi", names(Rhat))], main = "Rhat for xi")
hist(Rhat[grepl("phi", names(Rhat))], main = "Rhat for phi")
hist(Rhat, main="All parameters")
```

### Generating MVGP predictions

Using the elliptical slice sampler \citep{murray2010elliptical}, we generate posterior predictions for the unobserved average July temperature depth using the posterior samples estimated from the testate amoebae calibration model. These estimates can be generated either jointly with the calibration model or estimated using a two-stage approach as presented here. We have found, there is little practical difference in model performance between joint and two-stage estimation and follow the convention in the transfer function literature of using two-stage estimation.


```{r}
## Note, there might be some initial warnings about ESS shrunk to the
##    current position for the first few iterations. This is not a 
##    major concern as long as the warnings don't continue.
if (file.exists(paste0(save_directory, "predict-mvgp-pollen-no-analog2.RData"))) {
  load(paste0(save_directory, "predict-mvgp-pollen-no-analog2.RData"))
} else {
  pred <- predict_compositional_data(
    y_test, 
    X_train,
    params, 
    samples, 
    progress_directory, 
    "predict-mvgp-pollen-no-analog2.txt")
  save(pred, file = paste0(save_directory, "predict-mvgp-pollen-no-analog2.RData"))
}
```

### Plot MVGP predictions

The predictions for the chosen non-analog missing covariate observations are shown below. The held-out covariate values are shown in red dots and the posterior distribution shown as a violin plot where the width of the violin is proportional to the posterior density. The posterior mean is shown in blue and the posterior median is shown in orange. In general, the posterior distributions are doing a good job of estimating the unobserved covariate.

```{r, cache = TRUE}
## sorted to increasing values for ease of display
idx <- order(X_test)
## randomly choose 25 observations to display

n_samples <- length(pred$X[, 1])

sim_df <- data.frame(
  covariate   = c(pred$X[, idx] * sd_X + mean_X),
  mean        = rep(apply(pred$X[, idx] * sd_X + mean_X, 2, mean), 
                    each = n_samples),
  median      = rep(apply(pred$X[, idx], 2, median) * sd_X + mean_X,
                    each = n_samples),
  observation = factor(rep(1:length(analog_idx), each = n_samples )),
  truth       = rep(X_test[idx] * sd_X + mean_X,
                    each = n_samples))

ggplot(sim_df, aes(observation, covariate)) +
  geom_violin(position="identity") +
  geom_point(aes(observation, truth), color="red") +
  geom_point(aes(observation, mean), color="blue") +
  geom_point(aes(observation, median), color="orange") +
  scale_x_discrete(breaks=seq(5, length(analog_idx), 5)) + 
  labs(x="Observation", y="Predicted Average July Temperature") +
  ggtitle("MVGP prediction for pollen data")
```


## GAM model fit

We fit the GAM model for 200,000 MCMC iterations discarding the first 50,000 iterations as burn-in. We thin every 150 of the remaining 150,000 MCMC samples for 4 parallel chains resulting in 4,000 posterior samples. We used the default setting in `BayesComposition` resulting in a cubic B-spline model with 6 degrees of freedom.


```{r}
params <- list(
  n_adapt                    = 50000,
  n_mcmc                     = 150000,
  n_thin                     = 150, 
  likelihood                 = "dirichlet-multinomial", 
  function_type              = "basis",
  n_chains                   = 4,
  n_cores                    = 4)

## Fit no-analog using B-spline model
if (file.exists(paste0(save_directory, "fit-gam-pollen-no-analog.RData"))) {
  ## load mcmc
  load(paste0(save_directory, "fit-gam-pollen-no-analog.RData"))
} else {
  
  ## potentially long running MCMC code
  out <- fit_compositional_data(
    y                  = y_train,
    X                  = X_train,
    params             = params, 
    progress_directory = progress_directory,
    progress_file      = "fit-gam-pollen-no-analog.txt")
    
  save(out, file = paste0(save_directory, "fit-gam-pollen-no-analog.RData"))
}
```


```{r, echo=FALSE, include=FALSE}
Rhat <- make_gelman_rubin(out)
```

### GAM convergence diagnostics

After fitting the model, the posterior samples are generated and MCMC convergence is assessed using the Gelman-Rubin $\hat{R}$ statistic. We see the distribution of $\hat{R}$ statistics is close to 1 with the largest $\hat{R} = `r max(na.omit(Rhat))` \leq 1.1$ at an acceptable value (\citet{gelman2013bayesian}, pp.287). 

```{r, eval=TRUE}
## extract posterior samples
samples <- extract_compositional_samples(out)
beta_post <- samples$beta
alpha_post <- samples$alpha

n_samples <- nrow(beta_post)

## Calculate Gelman-Rubin convergence statistic
Rhat <- make_gelman_rubin(out)
layout(matrix(1:6, 3, 2))
hist(Rhat, main="All parameters")
hist(Rhat[grepl("beta", names(Rhat))], main = "Rhat for beta")
hist(Rhat[grepl("alpha", names(Rhat))], main = "Rhat for alpha")
Rhat[!is.finite(Rhat)] <- NA
max(unlist(na.omit(Rhat)))
```

### Generating GAM predictions

Using the elliptical slice sampler \citep{murray2010elliptical}, we generate posterior predictions for the unobserved average July temperature depth using the posterior samples estimated from the testate amoebae calibration model. These estimates can be generated either jointly with the calibration model or estimated using a two-stage approach as presented here. We have found, there is little practical difference in model performance between joint and two-stage estimation and follow the convention in the transfer function literature of using two-stage estimation.


```{r}
## Note, there might be some initial warnings about ESS shrunk to the
##    current position for the first few iterations. This is not a 
##    major concern as long as the warnings don't continue.
if (file.exists(paste0(save_directory, "predict-gam-pollen-no-analog.RData"))) {
  load(paste0(save_directory, "predict-gam-pollen-no-analog.RData"))
} else {
  pred_gam <- predict_compositional_data(
    y_test, 
    X_train,
    params, 
    samples, 
    progress_directory, 
    "predict-gam-pollen-no-analog.txt")
  save(pred_gam, file = paste0(save_directory, "predict-gam-pollen-no-analog.RData"))
}
```

### Plot GAM predictions

The predictions for the chosen non-analog missing covariate observations are shown below. The held-out covariate values are shown in red dots and the posterior distribution shown as a violin plot where the width of the violin is proportional to the posterior density. The posterior mean is shown in blue and the posterior median is shown in orange. In general, the posterior distributions are not doing as well at estimating the held-out non-analogs as the MVGP model.

```{r, cache = TRUE}
## sorted to increasing values for ease of display
idx <- order(X_test)
## randomly choose 25 observations to display

n_samples <- length(pred$X[, 1])

sim_df <- data.frame(
  covariate   = c(pred_gam$X[, idx] * sd_X + mean_X),
  mean        = rep(apply(pred_gam$X[, idx] * sd_X + mean_X, 2, mean), 
                    each = n_samples),
  median      = rep(apply(pred_gam$X[, idx], 2, median) * sd_X + mean_X,
                    each = n_samples),
  observation = factor(rep(1:length(analog_idx), each = n_samples )),
  truth       = rep(X_test[idx] * sd_X + mean_X,
                    each = n_samples))

ggplot(sim_df, aes(observation, covariate)) +
  geom_violin(position="identity") +
  geom_point(aes(observation, truth), color="red") +
  geom_point(aes(observation, mean), color="blue") +
  geom_point(aes(observation, median), color="orange") +
  scale_x_discrete(breaks=seq(5, length(analog_idx), 5)) + 
  labs(x="Observation", y="Predicted Average July Temperature") +
  ggtitle("GAM prediction for pollen data")
```


## BUMMER model fit to testate amoebae data

We fit the BUMMER model for 200,000 MCMC iterations discarding the first 50,000 iterations as burn-in. We thin every 150 of the remaining 150,000 MCMC samples for 4 parallel chains resulting in 4,000 posterior samples. 


```{r}
params <- list(
  n_adapt                    = 50000,
  n_mcmc                     = 150000,
  n_thin                     = 150, 
  likelihood                 = "dirichlet-multinomial", 
  function_type              = "bummer",
  n_chains                   = 4,
  n_cores                    = 4)


## Fit no-analog using bummer model
if (file.exists(paste0(save_directory, "fit-bummer-pollen-no-analog.RData"))) {

  ## load mcmc
  load(paste0(save_directory, "fit-bummer-pollen-no-analog.RData"))
} else {
  
  ## potentially long running MCMC code
  out <- fit_compositional_data(
    y                  = y_train,
    X                  = X_train,
    params             = params, 
    progress_directory = progress_directory,
    progress_file      = "fit-bummer-pollen-no-analog.txt")
    
  save(out, file = paste0(save_directory, "fit-bummer-pollen-no-analog.RData"))
}
```

```{r, echo=FALSE, include=FALSE}
Rhat <- make_gelman_rubin(out)
```

### BUMMER convergence diagnostics

After fitting the model, the posterior samples are generated and MCMC convergence is assessed using the Gelman-Rubin $\hat{R}$ statistic. We see the distribution of $\hat{R}$ statistics is close to 1 with the largest $\hat{R} = `r max(na.omit(Rhat))` \leq 1.1$ at an acceptable value (\citet{gelman2013bayesian}, pp.287). 


```{r, eval=TRUE}
## extract posterior samples
samples <- extract_compositional_samples(out)
a_post <- samples$a
b_post <- samples$b
c_post <- samples$c
alpha_post <- samples$alpha

n_samples <- nrow(a_post)

## Calculate Gelman-Rubin convergence statistic
Rhat <- make_gelman_rubin(out)
layout(matrix(1:6, 3, 2))
hist(Rhat, main="All parameters")
hist(Rhat[grepl("a\\b", names(Rhat))], main = "Rhat for a")
hist(Rhat[grepl("b", names(Rhat))], main = "Rhat for b")
hist(Rhat[grepl("c", names(Rhat))], main = "Rhat for c")
hist(Rhat[grepl("alpha", names(Rhat))], main = "Rhat for alpha")
Rhat[!is.finite(Rhat)] <- NA
max(unlist(na.omit(Rhat)))
```

### Generating BUMMER predictions

Using the elliptical slice sampler \citep{murray2010elliptical}, we generate posterior predictions for the unobserved average July temperature depth using the posterior samples estimated from the testate amoebae calibration model. These estimates can be generated either jointly with the calibration model or estimated using a two-stage approach as presented here. We have found, there is little practical difference in model performance between joint and two-stage estimation and follow the convention in the transfer function literature of using two-stage estimation.


```{r}
## Note, there might be some initial warnings about ESS shrunk to the
##    current position for the first few iterations. This is not a 
##    major concern as long as the warnings don't continue.
if (file.exists(paste0(save_directory, "predict-bummer-pollen-no-analog.RData"))) {
  load(paste0(save_directory, "predict-bummer-pollen-no-analog.RData"))
} else {
  pred_bummer <- predict_compositional_data(
    y_test, 
    X_train,
    params, 
    samples, 
    progress_directory, 
    "predict-bummer-pollen-no-analog.txt")
  save(pred_bummer, file = paste0(save_directory, "predict-bummer-pollen-no-analog.RData"))
}
```

### Plot BUMMER predictions

The predictions for the chosen non-analog missing covariate observations are shown below. The held-out covariate values are shown in red dots and the posterior distribution shown as a violin plot where the width of the violin is proportional to the posterior density. The posterior mean is shown in blue and the posterior median is shown in orange. In general, the posterior distributions are not doing as well at estimating the held-out non-analogs as the MVGP model.

```{r, cache = TRUE}
## sorted to increasing values for ease of display
idx <- order(X_test)
## randomly choose 25 observations to display

n_samples <- length(pred_bummer$X[, 1])

sim_df <- data.frame(
  covariate   = c(pred_bummer$X[, idx] * sd_X + mean_X),
  mean        = rep(apply(pred_bummer$X[, idx] * sd_X + mean_X, 2, mean), 
                    each = n_samples),
  median      = rep(apply(pred_bummer$X[, idx], 2, median) * sd_X + mean_X,
                    each = n_samples),
  observation = factor(rep(1:length(analog_idx), each = n_samples )),
  truth       = rep(X_test[idx] * sd_X + mean_X,
                    each = n_samples))

ggplot(sim_df, aes(observation, covariate)) +
  geom_violin(position="identity") +
  geom_point(aes(observation, truth), color="red") +
  geom_point(aes(observation, mean), color="blue") +
  geom_point(aes(observation, median), color="orange") +
  scale_x_discrete(breaks=seq(5, length(analog_idx), 5)) + 
  labs(x="Observation", y="Predicted Average July Temperature") +
  ggtitle("BUMMER prediction for pollen data")
```

## Fitting transfer function models to the non-analog testate amoebae data

Below, we fit the transfer function methods WA, MAT, and MLRC using the `R` `rioja` package.


```{r, message=FALSE, include=FALSE}
if (file.exists(paste0(save_directory, "other-models-testate-no-analog.RData"))) {
  load(paste0(save_directory, "other-models-pollen-no-analog")) 
} else {
  y_train_prop <- as.matrix(y_train_prop)
  
  ## WA reconstruction - subset to deal with all zero occurrence species
  zeros_idx <- which(apply(y_train_prop, 2, sum) == 0)
  if (length(zeros_idx) > 0) {
    modWA <- rioja::WA(y_train_prop[, - zeros_idx], X_train)
    predWA <- predict(modWA, y_test_prop[, - zeros_idx], sse=TRUE, nboot=1000)
  } else {
    ## no data to subset
    modWA <- rioja::WA(y_train_prop, X_train)
    predWA <- predict(modWA, y_test_prop, sse=TRUE, nboot=1000)      
  }
  
  pred_mu_WA <- predWA$fit[, 1]
  pred_sd_WA <- sqrt(predWA$v1.boot[, 1]^2 + predWA$v2.boot[1]^2)
  
  ## MLRC reconstruction - subset to deal with all zero occurrence species
  zeros_idx <- which(apply(y_train_prop, 2, sum) == 0)
  if (length(zeros_idx) > 0) {
    modMLRC <- rioja::MLRC(y_train_prop[, - zeros_idx], X_train)
    predMLRC <- predict(modMLRC, y_test_prop[, - zeros_idx],
                        sse=TRUE, nboot=1000)
  } else {
    modMLRC <- rioja::MLRC(y_train_prop, X_train)
    predMLRC <- predict(modMLRC, y_test_prop, sse=TRUE, nboot=1000)
  }
  
  pred_mu_MLRC <- predMLRC$fit[, 1]
  pred_sd_MLRC <- sqrt(predMLRC$v1.boot[, 1]^2 + predMLRC$v2.boot[1]^2)
  
  ## Modern analogue technique
  modMAT <- rioja::MAT(as.data.frame(y_train_prop), X_train, k=10, lean=FALSE)
  predMAT <- predict(modMAT, as.data.frame(y_test_prop), k=10, sse=TRUE, n.boot=1000)
  
  pred_mu_MAT <- predMAT$fit.boot[, 2]
  pred_sd_MAT <- sqrt(predMAT$v1.boot[, 2]^2+ predMAT$v2.boot[2])
  
  save(pred_mu_WA, pred_sd_WA, pred_mu_MLRC, pred_sd_MLRC, 
       pred_mu_MAT, pred_sd_MAT, 
       file = paste0(save_directory, "other-models-pollen-no-analog"))
}
```

### Generate non-analog predictive scores

After fitting all of the models under consideration, we compute the predictive scores for the pollen data below.

```{r}
models <- c("MVGP", "GAM", "BUMMER", "WA", "MAT", "MLRC")
n_models <- length(models)
N_no_analog <- length(analog_idx)
coverage <- matrix(0, N_no_analog, n_models)
MSPE <- matrix(0, N_no_analog, n_models)
MAE <- matrix(0, N_no_analog, n_models)
CRPS <- matrix(0, N_no_analog, n_models)

MSPE[, 1] <- (X_test - apply(pred$X, 2, mean))^2
MSPE[, 2] <- (X_test - apply(pred_gam$X, 2, mean))^2
MSPE[, 3] <- (X_test - apply(pred_bummer$X, 2, mean))^2
MSPE[, 4] <- (X_test - pred_mu_WA)^2
MSPE[, 5] <- (X_test - pred_mu_MAT)^2
MSPE[, 6] <- (X_test - pred_mu_MLRC)^2

MAE[, 1] <- abs(X_test - apply(pred$X, 2, median))
MAE[, 2] <- abs(X_test - apply(pred_gam$X, 2, median))
MAE[, 3] <- abs(X_test - apply(pred_bummer$X, 2, median))
MAE[, 4] <- abs(X_test - pred_mu_WA)
MAE[, 5] <- abs(X_test - pred_mu_MAT)
MAE[, 6] <- abs(X_test - pred_mu_MLRC)

coverage[, 1] <- 
  (X_test > apply(pred$X, 2, quantile, prob=0.025)) & 
  (X_test < apply(pred$X, 2, quantile, prob=0.975))
coverage[, 2] <-
  (X_test > apply(pred_gam$X, 2, quantile, prob=0.025)) &
  (X_test < apply(pred_gam$X, 2, quantile, prob=0.975))
coverage[, 3] <-
  (X_test > apply(pred_bummer$X, 2, quantile, prob=0.025)) &
  (X_test < apply(pred_bummer$X, 2, quantile, prob=0.975))
coverage[, 4] <- 
  (X_test > (pred_mu_WA - 2 * pred_sd_WA)) &
  (X_test < (pred_mu_WA + 2 * pred_sd_WA))
coverage[, 5] <- 
  (X_test > (pred_mu_MAT - 2 * pred_sd_MAT)) &
  (X_test < (pred_mu_MAT + 2 * pred_sd_MAT))
coverage[, 6] <-
  (X_test > (pred_mu_MLRC - 2 * pred_sd_MLRC)) &
  (X_test < (pred_mu_MLRC + 2 * pred_sd_MLRC))


CRPS[, 1] <- makeCRPS(pred$X, X_test, dim(pred$X)[1])
CRPS[, 2] <- makeCRPS(pred_gam$X, X_test, dim(pred_gam$X)[1])
CRPS[, 3] <- makeCRPS(pred_bummer$X, X_test, dim(pred_bummer$X)[1])
CRPS[, 4] <- MAE[, 4]
CRPS[, 5] <- MAE[, 5]
CRPS[, 6] <- MAE[, 6]

colnames(MSPE) <- models
colnames(MAE) <- models
colnames(coverage) <- models
colnames(CRPS) <- models


results <- rbind(
  apply(CRPS, 2, mean), apply(MSPE, 2, mean),
  apply(MAE, 2, mean), 100*apply(coverage, 2, mean))
rownames(results) <- c("CRPS", "MSPE", "MAE", "95% CI coverage")

# print(xtable(t(results), digits=4),
#       file=here("results", "appendix-pollen.tex"),
#       floating=FALSE)
```

```{r}
kable(t(results), digits = 4, format = "latex", longtable = FALSE)
```



